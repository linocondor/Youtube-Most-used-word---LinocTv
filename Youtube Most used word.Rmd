---
title: "Youtube Tending Videos - Most used Word"
output: html_notebook
---

This is a tutorial of the library from quanteda.

We are going to search for the most used word in Youtube trending videos comments from USA and UK

Github:


Dataset obtenido de #Kaggle:
https://lnkd.in/e66ir7S
 

```{r}
#install libraries
install.packages("quanteda")
install.packages("quanteda.textmodels")
install.packages("quanteda.textstats")
install.packages("quanteda.textplots")
```
```{r}
#Initiate library
library(quanteda)

```
```{r}
#summary 
#summary(data_corpus_inaugural)
data = data_corpus_inaugural  #make a copy of ddata
summary(data, n = 5)

#head(data)
```

```{r}
#plot summary of data
tokeninfo <- summary(data_corpus_inaugural)
tokeninfo$Year <- docvars(data_corpus_inaugural, "Year")
if (require(ggplot2)) ggplot(data = tokeninfo, aes(x = Year, y = Tokens, group = 1)) + 
    geom_line() + geom_point() + scale_x_continuous(labels = c(seq(1789, 2017, 12)), 
    breaks = seq(1789, 2017, 12)) + theme_bw()
```
```{r}
# Longest inaugural address: William Henry Harrison
tokeninfo[which.max(tokeninfo$Tokens), ]
```
```{r}

data_tokens_inaugural <- tokens(data_corpus_inaugural)
data_tokens_inaugural[1]
kwic(data_tokens_inaugural, pattern = "terror")
```
```{r}
kwic(data_tokens_inaugural, pattern = "terror", valuetype = "regex")
```
```{r}
#tokenizing sentences --------------
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!", 
    text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
a <- tokens(txt)
print(a)
```
```{r}
corp_inaug_post1990 <- corpus_subset(data_corpus_inaugural, Year > 1990)

# make a dfm
dfmat_inaug_post1990 <- tokens(corp_inaug_post1990) %>% dfm()
dfmat_inaug_post1990[, 1:5]
```

```{r}
# make a dfm, removing stopwords and applying stemming
dfmat_inaug_post1990 <- dfm(dfmat_inaug_post1990, remove = stopwords("english"), 
    stem = TRUE, remove_punct = TRUE)
dfmat_inaug_post1990[, 1:5]
```
```{r}
dfmat_uk <- tokens(data_char_ukimmig2010, remove_punct = TRUE) %>% tokens_remove(stopwords("en")) %>% 
    dfm()
dfmat_uk
topfeatures(dfmat_uk, 20)  # 20 most frequent words
```
```{r}
set.seed(100)
install.packages("quanteda.textplots")
library(quanteda.textplots)
textplot_wordcloud(dfmat_uk, min_count = 6, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```
```{r}
if (require("quanteda.textmodels")) {
    dfmat_ire <- dfm(tokens(data_corpus_irishbudget2010))
    tmod_wf <- textmodel_wordfish(dfmat_ire, dir = c(2, 1))

    # plot the Wordfish estimates by party
    textplot_scale1d(tmod_wf, groups = docvars(dfmat_ire, "party"))
}
dfmat_ire <- dfm(tokens(data_corpus_irishbudget2010))
dfmat_ire

tmod_wf <- textmodel_wordfish(dfmat_ire, dir = c(2, 1))
tmod_wf

quant_dfm <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE, remove_numbers = TRUE) %>% 
    tokens_remove(stopwords("en")) %>% dfm()
quant_dfm <- dfm_trim(quant_dfm, min_termfreq = 4, max_docfreq = 10)
quant_dfm
```
```{r}
#install.packages("stm")
library(stm)
set.seed(100)
if (require("stm")) {
    my_lda_fit20 <- stm(quant_dfm, K = 20, verbose = FALSE)
    plot(my_lda_fit20)
}
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

